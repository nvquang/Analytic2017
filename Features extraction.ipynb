{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#recognize words: not \"-', don't make difference between some similar words\n",
    "#maybe use third parties vectorizer :)\n",
    "#after exlusion of weird words ^^\n",
    "#requires build a list of only words first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a0ad6f1f15dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clarity'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'conciseness'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clarity'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'conciseness'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"data/training/data_train.csv\", dtype='unicode')\n",
    "data_df.fillna(value='', inplace=True)\n",
    "data_df = data_df.apply(lambda x: x.astype(str).str.lower())\n",
    "data_df[['clarity','conciseness']] = data_df[['clarity','conciseness']].apply(pd.to_numeric)\n",
    "train_df, test_df = train_test_split(data_df, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getListCategories(train_df):\n",
    "    listCat=[]\n",
    "    for i in range(1,len (train_df)):\n",
    "        category = str(train_df['category_lvl_1'][i])+\"/\"+str(train_df['category_lvl_2'][i])+\"/\"+str(train_df['category_lvl_3'][i])\n",
    "        if category not in listCat:\n",
    "            listCat.append(category)\n",
    "        \n",
    "    print(\"number of categories\")\n",
    "    print(len(listCat))\n",
    "    return listCat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def special_match(strg, search=re.compile(r'[^a-zA-Z0-9.]').search):\n",
    "    return not bool(search(strg))\n",
    "\n",
    "def is_code(word):\n",
    "    if (not word.isalpha()) & (not word.isdigit()) & (special_match(word)) &(not is_measure(word)) :\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def compressSeq(token):\n",
    "    string=''\n",
    "    for i in range(len(token)):\n",
    "        if token[i].isdigit():\n",
    "            string+='0'\n",
    "        elif token[i].isalpha():\n",
    "            string+='a'                        \n",
    "        elif token[i] in [\"?\",\".\", \";\", \",\", \"!\",\":\"]:\n",
    "            string+='.'\n",
    "        else:\n",
    "            string+='?'\n",
    "        \n",
    "    string = string.replace(\"0''\", \"0aa\" )\n",
    "    i=0\n",
    "    while (i<len(string)-1):\n",
    "        if string[i] == string[i+1]:\n",
    "            string = string.replace(string[i]+string[i+1], string[i])\n",
    "        else:\n",
    "            i+=1                             \n",
    "    return string\n",
    "\n",
    "def is_measure(word):\n",
    "    if (not word.isalpha()) & (not word.isdigit()):\n",
    "        if compressSeq(word) == \"0a\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def preProc(string, lem):\n",
    "    string = string.replace('-','')\n",
    "    \n",
    "    words = string.split(\" \")\n",
    "    string2=''\n",
    "    for word in words:\n",
    "        if word.isalpha():\n",
    "            word = word.lower()\n",
    "            string2 += lem.lemmatize(word)+\" \"\n",
    "            \n",
    "    return string2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getListVocabulary(data, lem, lenMin = 2, maxSize =5000):           \n",
    "    dictionary = []\n",
    "    frequency = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #1 collect all words and number of appear\n",
    "    for text in data:\n",
    "        for word in  preProc(text, lem).split(\" \"):\n",
    "            if (len(word) >= lenMin)  :\n",
    "                if word not in dictionary:\n",
    "                    dictionary.append(word)\n",
    "                    frequency.append(1)\n",
    "                else:\n",
    "                    index=0\n",
    "                    while dictionary[index] != word:\n",
    "                        index+=1\n",
    "                    frequency[index]+=1\n",
    "                    \n",
    "    #2 order words by their number of appear\n",
    "                \n",
    "    tempWord=''\n",
    "    tempNum=0\n",
    "    \n",
    "    for i in range(len(dictionary)):\n",
    "        for j in range(i, len(dictionary)):\n",
    "            if frequency[i] < frequency[j]:\n",
    "                tempWord = dictionary[i]\n",
    "                dictionary[i] = dictionary[j]\n",
    "                dictionary[j] = tempWord\n",
    "                tempNum = frequency[i]\n",
    "                frequency[i] = frequency[j]\n",
    "                frequency[j] = tempNum\n",
    "     \n",
    "    #3 select sizeMax \n",
    "    newList=[]\n",
    "    for i in range(maxSize):\n",
    "        newList.append(dictionary[i])\n",
    "            \n",
    "    \n",
    "    print(\"size of dictionnary\")\n",
    "    print(len(newList))\n",
    "    print(\"number of words ignored:\")\n",
    "    print(len(dictionary) - len(newList))\n",
    "    return newList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "listVoc=getListVocabulary(train_df['title'], lem)\n",
    "listCat=getListCategories(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embeddVector(title, category, listVoc, listCat, lem):    \n",
    "    voc = preProc(title, lem).split(\" \")\n",
    "    vector=np.zeros((len(listVoc) +  len(listCat) + 3))\n",
    "    \n",
    "    for i in range(len(listVoc)):\n",
    "        for word in voc:\n",
    "            if word == listVoc[i]:\n",
    "                vector[i] = 1\n",
    "                \n",
    "    for i in range(len(listCat)):\n",
    "        if category == listCat[i]:\n",
    "            vector[len(listVoc) + i] = 1\n",
    "        \n",
    "    words = title.split(\" \")\n",
    "    nbWords=len(words)\n",
    "    nbCodes = 0\n",
    "    nbMeasures=0\n",
    "    for word in words:\n",
    "        if is_code(word):\n",
    "            nbCodes+=1\n",
    "            \n",
    "        \n",
    "        if is_measure(word):\n",
    "            nbMeasures+=1\n",
    "            \n",
    "    vector[len(listVoc) +  len(listCat) ] = nbWords\n",
    "    vector[len(listVoc) +  len(listCat) +1] = nbCodes\n",
    "    vector[len(listVoc) +  len(listCat) +2] = nbMeasures\n",
    "    \n",
    "    return vector    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorizeLabelledData(train_df):\n",
    "\n",
    "    print(\"embedding  data\")\n",
    "    train_vect=[]\n",
    "    label_clar_vect=[]\n",
    "    label_conc_vect=[]\n",
    "\n",
    "    for i in range(len(train_df)):\n",
    "    #for i in range(100):\n",
    "\n",
    "        title = train_df['title'][i]\n",
    "        category = str(train_df['category_lvl_1'][i])+\"/\"+ str(train_df['category_lvl_2'][i])+\"/\"+ str(train_df['category_lvl_3'][i])  \n",
    "        train_vect.append(embeddVector(title, category, listVoc, listCat, lem))\n",
    "    \n",
    "        label_clar_vect.append(train_df['clarity'][i])\n",
    "        label_conc_vect.append(train_df['conciseness'][i])\n",
    "    \n",
    "        if i % (len(train_df)/100 ) == 0 :\n",
    "            print( str(float(i) / float(len(train_df))))\n",
    "\n",
    "    print('finished: '+str(i+1)+' samples')\n",
    "    train_vect = np.matrix(train_vect)\n",
    "    label_clar_vect = np.asarray(label_clar_vect)\n",
    "    label_conc_vect = np.asarray(label_conc_vect)\n",
    "\n",
    "    return(train_vect, label_clar_vect, label_conc_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train_vect, label_clar_vect, label_conc_vect) = vectorizeLabelledData(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=0, splitter='best')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (\"Training the random forest...\")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0, nb_estimators = 100)\n",
    "clf.fit(train_vect,label_clar_vect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(train_vect,label_clar_vect)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
