{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#recognize words: not \"-', don't make difference between some similar words\n",
    "#maybe use third parties vectorizer :)\n",
    "#after exlusion of weird words ^^\n",
    "#requires build a list of only words first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "%matplotlib inline\n",
    "train_df = pd.read_csv(\"data/training/data_train.csv\", dtype='unicode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getListCategories(train_df):\n",
    "    listCat=[]\n",
    "    for i in range(1,len (train_df)):\n",
    "        category = str(train_df['category_lvl_1'][i])+\"/\"+str(train_df['category_lvl_2'][i])+\"/\"+str(train_df['category_lvl_3'][i])\n",
    "        if category not in listCat:\n",
    "            listCat.append(category)\n",
    "        \n",
    "    print(\"number of categories\")\n",
    "    print(len(listCat))\n",
    "    return listCat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def special_match(strg, search=re.compile(r'[^a-zA-Z0-9.]').search):\n",
    "    return not bool(search(strg))\n",
    "\n",
    "def is_code(word):\n",
    "    if (not word.isalpha()) & (not word.isdigit()) & (special_match(word)) &(not is_measure(word)) :\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def compressSeq(token):\n",
    "    string=''\n",
    "    for i in range(len(token)):\n",
    "        if token[i].isdigit():\n",
    "            string+='0'\n",
    "        elif token[i].isalpha():\n",
    "            string+='a'                        \n",
    "        elif token[i] in [\"?\",\".\", \";\", \",\", \"!\",\":\"]:\n",
    "            string+='.'\n",
    "        else:\n",
    "            string+='?'\n",
    "        \n",
    "    string = string.replace(\"0''\", \"0aa\" )\n",
    "    i=0\n",
    "    while (i<len(string)-1):\n",
    "        if string[i] == string[i+1]:\n",
    "            string = string.replace(string[i]+string[i+1], string[i])\n",
    "        else:\n",
    "            i+=1                             \n",
    "    return string\n",
    "\n",
    "def is_measure(word):\n",
    "    if (not word.isalpha()) & (not word.isdigit()):\n",
    "        if compressSeq(word) == \"0a\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def process_word(token):\n",
    "    token = token.lower()\n",
    "    if token[len(token)-1] == 's':\n",
    "        token = token[:len(token)-1] \n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getListVocabulary(data):    \n",
    "    #Build a dictionnary with gensim: every word with id associated\n",
    "    #First remove all codes and measurements and make a list of string with the remaining\n",
    "        \n",
    "    texts = [[word.lower() for word in title.split(\" \") if process_word(word) == 1 ] for title in train_df['title'] ]\n",
    "    \n",
    "    dictionary = gensim.corpora.Dictionary(texts)\n",
    "    dictionary.save('dict')  \n",
    "                \n",
    "    print(\"size of dictionnary\")\n",
    "    print(len(dictionary))\n",
    "    return dictionary\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of dictionnary\n",
      "19633\n",
      "number of categories\n",
      "204\n"
     ]
    }
   ],
   "source": [
    "listVoc=getListVocabulary(train_df)\n",
    "listCat=getListCategories(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def embeddVector(train_df, i, listVoc, listCat):\n",
    "    \n",
    "    title = train_df['title'][i]\n",
    "    category = str(train_df['category_lvl_1'][i])+\"/\"+ str(train_df['category_lvl_2'][i])+\"/\"+ str(train_df['category_lvl_3'][i])\n",
    "    words = title.split(\" \")\n",
    "    size = len(listVoc) +  len(listCat)\n",
    "    vector=np.zeros((size + 2))\n",
    "    for i in range(len(listVoc)):\n",
    "        for word in words:\n",
    "            if process_word(word) == listVoc[i]:\n",
    "                vector[i] = 1\n",
    "                \n",
    "    for i in range(len(listCat)):\n",
    "        if category == listCat[i]:\n",
    "            vector[len(listVoc) + 1 + i] = 1\n",
    "        \n",
    "    \n",
    "    nbCodes = 0\n",
    "    nbMeasures=0\n",
    "    for word in words:\n",
    "        if is_code(word):\n",
    "            nbCodes+=1\n",
    "            \n",
    "        \n",
    "        if is_measure(word):\n",
    "            nbMeasures+=1\n",
    "            \n",
    "            \n",
    "    vector[size +1] = nbCodes\n",
    "    vector[size ] = nbMeasures\n",
    "    return vector    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country              sku_id  \\\n",
      "0      my    AD674FAASTLXANMY   \n",
      "1      my  AE068HBAA3RPRDANMY   \n",
      "2      my  AN680ELAA9VN57ANMY   \n",
      "3      my  AN957HBAAAHDF4ANMY   \n",
      "4      my    AR511HBAXNWAANMY   \n",
      "5      my    AS575ELCMZ4WANMY   \n",
      "6      my  AS727ELAA9LLV1ANMY   \n",
      "7      my  BU512HBAA4WUVTANMY   \n",
      "8      my    CL787ELAW29LANMY   \n",
      "9      my  CO633HLAABREKOANMY   \n",
      "\n",
      "                                               title  \\\n",
      "0       Adana Gallery Suri Square Hijab – Light Pink   \n",
      "1  Cuba Heartbreaker Eau De Parfum Spray 100ml/3.3oz   \n",
      "2  Andoer 150cm Cellphone Smartphone Mini Dual-He...   \n",
      "3  ANMYNA Complaint Silky Set 柔顺洗发配套 (Shampoo 520...   \n",
      "4  Argital Argiltubo Green Clay For Face and Body...   \n",
      "5  Asus TP300LJ-DW004H Transformer Book Flip 4GB ...   \n",
      "6  NG-40C Ring-Shaped 40W 3166lm 5400K Macro Phot...   \n",
      "7            Buytra Exfoliating Peel Foot Mask 1Pair   \n",
      "8  CLiPtec OCC121 Slim Flat USB 3.0 Extension Cab...   \n",
      "9  McDonald's Coke Can Glass Limited Edition 12oz...   \n",
      "\n",
      "                          category_lvl_1      category_lvl_2  \\\n",
      "0                                Fashion               Women   \n",
      "1                        Health & Beauty         Bath & Body   \n",
      "2  TV, Audio / Video, Gaming & Wearables               Audio   \n",
      "3                        Health & Beauty           Hair Care   \n",
      "4                        Health & Beauty          Men's Care   \n",
      "5                    Computers & Laptops             Laptops   \n",
      "6                                Cameras  Camera Accessories   \n",
      "7                        Health & Beauty         Bath & Body   \n",
      "8                    Computers & Laptops             Laptops   \n",
      "9                          Home & Living    Kitchen & Dining   \n",
      "\n",
      "                category_lvl_3  \\\n",
      "0                  Muslim Wear   \n",
      "1             Hand & Foot Care   \n",
      "2           Live Sound & Stage   \n",
      "3      Shampoos & Conditioners   \n",
      "4           Body and Skin Care   \n",
      "5          Traditional Laptops   \n",
      "6  Lighting & Studio Equipment   \n",
      "7             Hand & Foot Care   \n",
      "8          Traditional Laptops   \n",
      "9                    Tableware   \n",
      "\n",
      "                                   short_description   price   product_type  \\\n",
      "0  <ul><li>Material : Non sheer shimmer chiffon</...      49          local   \n",
      "1  Formulated with oil-free hydrating botanicals/...     128  international   \n",
      "2  <ul> <li>150cm mini microphone compatible for ...   25.07  international   \n",
      "3  <ul> <li>ANMYNA Complaint Silky Set (Shampoo 5...     118          local   \n",
      "4  <ul> <li>100% Authentic</li> <li>Rrefresh and ...   114.8  international   \n",
      "5  <div class=\"prod_content\"> <div class=\"prod_de...    2599          local   \n",
      "6  <ul> <li>1. Color Temperature: 5400K</li> <li>...  388.99  international   \n",
      "7  <ul> <li>Reviving like a new born baby.</li> <...    10.4  international   \n",
      "8  <ul style= \"padding: 0px; margin: 20px 0px 0px...      29          local   \n",
      "9  <ul> <li>Genuine issued McDonald's Coca Cola m...      25          local   \n",
      "\n",
      "  clarity conciseness  \n",
      "0       1           1  \n",
      "1       1           1  \n",
      "2       1           0  \n",
      "3       1           1  \n",
      "4       1           1  \n",
      "5       1           1  \n",
      "6       1           1  \n",
      "7       1           1  \n",
      "8       1           1  \n",
      "9       1           1  \n",
      "[ 1.  1.  1. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  2.  1.]\n",
      "[ 0.  0.  0. ...,  0.  1.  0.]\n",
      "[ 0.  0.  0. ...,  0.  1.  0.]\n",
      "[ 0.  0.  0. ...,  0.  2.  1.]\n",
      "[ 0.  0.  1. ...,  0.  3.  0.]\n",
      "[ 0.  0.  0. ...,  0.  1.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  3.]\n",
      "[ 0.  0.  0. ...,  0.  1.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  1.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  1.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  1.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  1.  0.]\n",
      "[ 0.  0.  0. ...,  0.  1.  1.]\n",
      "[ 0.  0.  0. ...,  0.  0.  2.]\n",
      "[ 0.  0.  0. ...,  0.  0.  1.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  1.  0.]\n",
      "[ 0.  0.  1. ...,  0.  0.  1.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  1.  2.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  1.]\n",
      "[ 0.  0.  0. ...,  0.  0.  2.]\n",
      "[ 0.  0.  0. ...,  0.  0.  1.]\n",
      "[ 0.  0.  0. ...,  0.  0.  2.]\n",
      "[ 0.  0.  0. ...,  0.  3.  4.]\n",
      "[ 0.  0.  0. ...,  0.  0.  1.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head(10))\n",
    "for i in range(50):\n",
    "    print(embeddVector(train_df, i, listVoc, listCat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-761eedf1bb8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<.*?>'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_html\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'short_description_clear_html'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcleanhtml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'short_description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data_df' is not defined"
     ]
    }
   ],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    result = re.sub(re.compile('<.*?>'), ' ', raw_html)\n",
    "    return result\n",
    "data_df['short_description_clear_html'] = data_df.apply(lambda row: cleanhtml(row['short_description']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19633\n"
     ]
    }
   ],
   "source": [
    "print(len(listVoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.corpora.dictionary.Dictionary'>\n"
     ]
    }
   ],
   "source": [
    "print(type(listVoc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
